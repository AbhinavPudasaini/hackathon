{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJrFAcdJcsRYkk7GCrD9zy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhinavPudasaini/hackathon/blob/main/Loan_Credit_scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x0ixiuseHEN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "import shap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# These dataset are from loan default credit scoring datat science competiton"
      ],
      "metadata": {
        "id": "051M_3AsMj3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datat = pd.read_csv('/content/Train1.csv')\n",
        "datae = pd.read_csv('/content/Test (1).csv')"
      ],
      "metadata": {
        "id": "c1PcRxHceVLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datat.info()"
      ],
      "metadata": {
        "id": "Ksf3Q9vNMX31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datat.describe()"
      ],
      "metadata": {
        "id": "iSHsE4ZOMbMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down in the code when i ched the feature importance using SHAP, these columns were the most ireelevant for the model training and formation."
      ],
      "metadata": {
        "id": "Hh4rl9JDM7k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datat.drop(['duration', 'lender_id', 'New_versus_Repeat','ID', 'customer_id', 'country_id','tbl_loan_id'],axis=1,inplace=True)\n",
        "datae.drop(['duration', 'lender_id', 'New_versus_Repeat','ID', 'customer_id', 'country_id','tbl_loan_id'],axis=1,inplace=True)\n",
        "datat.drop(['disbursement_date','due_date'],axis=1, inplace = True)\n",
        "datae.drop(['disbursement_date','due_date'],axis=1, inplace = True)"
      ],
      "metadata": {
        "id": "40pntp7FhWkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also tried creating different columns, like ration between total and total amount to repay so make good prediction."
      ],
      "metadata": {
        "id": "Glftxh46NIvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datat['total_amount_ratio'] = datat['Total_Amount_to_Repay'] / datat['Total_Amount']\n",
        "datae['total_amount_ratio'] = datae['Total_Amount_to_Repay'] / datae['Total_Amount']\n",
        "datat['Other_amount_for_loan'] = datat['Total_Amount_to_Repay'] - datat['Lender_portion_to_be_repaid']\n",
        "datae['Other_amount_for_loan'] = datae['Total_Amount_to_Repay'] - datae['Lender_portion_to_be_repaid']\n",
        "datat['left_amount_for_loan'] = datat['Total_Amount'] - datat['Amount_Funded_By_Lender']\n",
        "datae['left_amount_for_loan'] = datae['Total_Amount'] - datae['Amount_Funded_By_Lender']"
      ],
      "metadata": {
        "id": "FNyPRqaiRTXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used mainly catboost so i didn't needed to transform categorical variable like this cause catboost can directly encode them. But catboost mainly use one-hot-encoding which increase dimension unnecessarily so i used labelencoder.In tree base algorithms using label encoder in categorical varibale will not affect."
      ],
      "metadata": {
        "id": "O5I2eoi9NrW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "custom_order = ['Type_1', 'Type_2', 'Type_3', 'Type_4','Type_5','Type_6','Type_7','Type_8','Type_9','Type_10','Type_11','Type_12','Type_13','Type_14','Type_15','Type_16','Type_17','Type_18','Type_19','Type_20','Type_21','Type_22','Type_23', 'Type_24']  # Desired order\n",
        "le = LabelEncoder()\n",
        "le.fit(custom_order)\n",
        "# le.fit(datat['loan_type'])\n",
        "datat['loan_type'] = le.transform(datat['loan_type'])\n",
        "datae['loan_type'] = le.transform(datae['loan_type'])"
      ],
      "metadata": {
        "id": "kzyLHKkxVJdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = datat.drop(['target'],axis=1)\n",
        "y_train = datat['target']\n",
        "X_test = datae"
      ],
      "metadata": {
        "id": "RKNGyuudVXRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data and mainly target variable is highly imbalance, I computed the class weight and then tried balancing to get better results."
      ],
      "metadata": {
        "id": "ZhknY7OoOam2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(datat['target']), y=datat['target'])\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "uJGES7i1Vq4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used optuna to get the best hyperparameter."
      ],
      "metadata": {
        "id": "_dZgDMEdOudc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def objective(trial):\n",
        "    iterations = trial.suggest_int(\"iterations\", 1000, 8000)\n",
        "    depth = trial.suggest_int(\"depth\", 4, 10)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
        "    l2_leaf_reg = trial.suggest_float(\"l2_leaf_reg\", 2, 8)\n",
        "    colsample_bylevel = trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0)\n",
        "    class_weights = [0.5093329 , 25.18696343]\n",
        "\n",
        "    # Initializing the CatBoost model with the suggested hyperparameters\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=iterations,\n",
        "        depth=depth,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        colsample_bylevel=colsample_bylevel,\n",
        "        eval_metric='F1',\n",
        "        random_seed=42,\n",
        "        class_weights=class_weights,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=20, verbose=0)\n",
        "\n",
        "    y_pred = model.predict(X_valid)\n",
        "\n",
        "    f1 = f1_score(y_valid, y_pred)\n",
        "    return f1\n"
      ],
      "metadata": {
        "id": "C1eLaKh8UxjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "# Start optimization\n",
        "# You can adjust the number of trials\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "id": "7LUHrTvnWCMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example weights for a binary classification\n",
        "#This is the result i got from above compute class weight function\n",
        "class_weights = [0.5093329 , 27.18696343]\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=40,\n",
        "    learning_rate=0.106077355116654,\n",
        "    depth=10,\n",
        "    class_weights=class_weights,\n",
        "    eval_metric='F1',\n",
        "    random_seed=42,\n",
        "    l2_leaf_reg = 8.171923654908364,\n",
        "    colsample_bylevel =0.5733702387068401,\n",
        "    early_stopping_rounds=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train,eval_set=(X_valid, y_valid))"
      ],
      "metadata": {
        "id": "4oNKBmGGHPUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = model.predict(datae)\n",
        "sample = pd.read_csv(\"/content/SampleSubmission.csv\")\n",
        "new_data = pd.DataFrame({'ID': sample['ID'], 'target': y_test})\n",
        "new_data.to_csv('submission_new_03.csv', index=False)"
      ],
      "metadata": {
        "id": "ErxkEVYnJSuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Below i have done experiment with different models too like xgboost, randomforest, svm ,logistic regression. Svm actually didn't run or we can say that it took long long time because the dataset contains almost 70000 datas."
      ],
      "metadata": {
        "id": "LyOSrL_tQrxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.1, random_state=42)\n"
      ],
      "metadata": {
        "id": "ZUxWYxkUwK0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I also did some experiments with Xgboost::"
      ],
      "metadata": {
        "id": "S6I-7r3NPzJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "# Example weights for a binary classification\n",
        "class_weights = [0.5093329, 27.18696343]  # Adjust based on dataset imbalance\n",
        "\n",
        "# Compute sample weights for each instance in the training set based on class weights\n",
        "# sample_weights = compute_sample_weight(class_weight={0: class_weights[0], 1: class_weights[1]}, y=y_train)\n",
        "\n",
        "model_xgb = xgb.XGBClassifier(\n",
        "    n_estimators=5000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample = 0.7,\n",
        "    scale_pos_weight=class_weights[1] / class_weights[0],  # Adjust class imbalance scaling\n",
        "    eval_metric='logloss',  # For binary classification, you can also use 'logloss' or 'error'\n",
        "    random_state=42,\n",
        "    reg_lambda=4,  # L2 regularization (Ridge)\n",
        "    reg_alpha=0.1 # L1 regularization (Lasso)    colsample_bytree=0.83,\n",
        ")\n",
        "\n",
        "model_xgb.fit(X_train, y_train, verbose=100)\n"
      ],
      "metadata": {
        "id": "SKbQ-0u2ebDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used SHAP to get the best feature and also improve score."
      ],
      "metadata": {
        "id": "Loljy5gtP7G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_valid)\n"
      ],
      "metadata": {
        "id": "iV43mU_zrJFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary plot for all SHAP values\n",
        "shap.summary_plot(shap_values, X_valid)\n"
      ],
      "metadata": {
        "id": "LvPkHKQAxJh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, X_valid, plot_type=\"bar\")\n"
      ],
      "metadata": {
        "id": "tYFw7oQlxQg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(class_weight=class_weights)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "3nFaXBvuBoKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define custom binning logic\n",
        "def loan_type_bins(value):\n",
        "    if value in [\"Type_14\",\n",
        "\"Type_2\"\t,\n",
        "\"Type_11\"\t,\n",
        "\"Type_18\"\t,\n",
        "\"Type_17\"\t,\n",
        "\"Type_12\"\t,\n",
        "\"Type_23\"\t,\n",
        "\"Type_20\"\t,\n",
        "\"Type_16\"\t,\n",
        "\"Type_13\"\t,\n",
        "\"Type_19\"\t,\n",
        "\"Type_15\"\t,\n",
        "\"Type_21\"\t,\n",
        "\"Type_24\"\t,\n",
        "\"Type_22\"]:  # Example group\n",
        "        return 'Type_1_100'\n",
        "    elif value in [\"Type_10\",\t\"Type_6\"\t,\"Type_9\"\t]:  # Another group\n",
        "        return 'Type_100_500'\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "# Apply the function to create a new column\n",
        "datat['loan_type_bins'] = datat['loan_type'].apply(loan_type_bins)\n",
        "datae['loan_type_bins'] = datae['loan_type'].apply(loan_type_bins)\n",
        "\n",
        "\n",
        "# Preview data\n",
        "# print(df[['loan_type', 'loan_type_bin']].head())\n"
      ],
      "metadata": {
        "id": "Yp6loSToikug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.get_dummies(datat['loan_type']).astype(int)\n",
        "dataee = pd.get_dummies(datae['loan_type']).astype(int)"
      ],
      "metadata": {
        "id": "xLDZSI298MP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datat = pd.concat([datat, data], axis=1)\n",
        "# datae = pd.concat([datae, dataee], axis=1)"
      ],
      "metadata": {
        "id": "1DEpySvA8lSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'iterations': [1000, 1250, 1500],   # Number of trees\n",
        "    'learning_rate': [0.01, 0.05, 0.1],  # Step size for updating weights\n",
        "    'depth': [6, 8, 10],              # Depth of each tree\n",
        "    'l2_leaf_reg': [1, 3, 5],        # L2 regularization\n",
        "}"
      ],
      "metadata": {
        "id": "SMPdeutfS5PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CatBoostClassifier(\n",
        "    cat_features=['loan_type','New_versus_Repeat'],\n",
        "    loss_function='Logloss',     # For binary classification\n",
        "    eval_metric='Accuracy',      # Evaluation metric\n",
        "    verbose=False                # Suppress output during training\n",
        ")\n"
      ],
      "metadata": {
        "id": "a1lv0QiqTDf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=10,\n",
        "    scoring='accuracy',         # Adjust based on your metric\n",
        "    cv=3,                       # 3-fold cross-validation\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        "\n",
        ")\n",
        "\n",
        "# Fit Grid Search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "# Best Parameters: {'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 1000, 'depth': 6, 'border_count': 64}"
      ],
      "metadata": {
        "id": "RZE032YPTMTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Best Parameters: {'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 1000, 'depth': 6, 'border_count': 64}\n"
      ],
      "metadata": {
        "id": "gjaJ6Z-bRxcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel=\"rbf\", gamma=0.5, C=1.0)\n",
        "# Trained the model\n",
        "svm.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "DWXz9_n7YY5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vYYGwvVcJ4JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = datat[datat['target'] == 1]\n",
        "new_df1 = datat.sample(35000)\n",
        "df = pd.concat([new_df, new_df1], axis=0, ignore_index=True)\n",
        "X_train = datat.drop(['target'],axis=1)\n",
        "y_train = datat['target']\n",
        "\n",
        "X_test = datae"
      ],
      "metadata": {
        "id": "9bFDwYd6M7JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CatBoostClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "cat_features=['loan_type','New_versus_Repeat'], l2_leaf_reg = 4,\n",
        "model = CatBoostClassifier(iterations=3000, learning_rate=0.05, depth=6,colsample_bylevel = 0.8 ,early_stopping_rounds=1300)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "vgx5eCjcNsCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lee = LabelEncoder()\n",
        "\n",
        "custom_order = [\"Type_1\",\n",
        "\"Type_2\"\t,\n",
        "\"Type_3\"\t,\n",
        "\"Type_4\"\t,\n",
        "\"Type_5\"\t,\n",
        "\"Type_6\"\t,\n",
        "\"Type_7\"\t,\n",
        "\"Type_8\"\t,\n",
        "\"Type_9\"\t,\n",
        "\"Type_10\"\t,\n",
        "\"Type_11\"\t,\n",
        "\"Type_12\"\t,\n",
        "\"Type_13\"\t,\n",
        "\"Type_14\"\t,\n",
        "\"Type_15\"\t,\n",
        "\"Type_16\"\t,\n",
        "\"Type_17\"\t,\n",
        "\"Type_18\",\n",
        "\"Type_19\",\n",
        "\"Type_20\",\n",
        "\"Type_21\",\n",
        "\"Type_22\",\n",
        "\"Type_23\",\n",
        "\"Type_24\"]\n",
        "\n",
        "lee.fit(custom_order)\n",
        "datat['loan_type'] = lee.transform(datat['loan_type'])\n",
        "\n",
        "datae['loan_type'] = lee.transform(datae['loan_type'])\n"
      ],
      "metadata": {
        "id": "t39mz20WdojN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = datat.drop(['target'],axis=1)\n",
        "y_train = datat['target']\n",
        "X_test = datae"
      ],
      "metadata": {
        "id": "Cqb93zRwmR5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "# model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# y_pry_tested = model.predict(X_test)\n",
        "\n",
        "# Convert to XGBoost's DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test)  # Test data doesn't need labels\n"
      ],
      "metadata": {
        "id": "QhA5Rcr7YBg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'objective': 'binary:logistic',   # For binary classification\n",
        "    'max_depth': 25,\n",
        "    'learning_rate': 0.07,\n",
        "    'n_estimators': 500,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0.1,\n",
        "    'reg_lambda': 1,\n",
        "    'reg_alpha': 0.5,\n",
        "    'eval_metric': 'logloss'\n",
        "}\n"
      ],
      "metadata": {
        "id": "oruhmCfbiGaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_boost_round = 500\n",
        "model = xgb.train(params, dtrain, num_boost_round)\n"
      ],
      "metadata": {
        "id": "qL6DmndujlU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob = model.predict(dtest)\n",
        "y_pred = [1 if prob > 0.45 else 0 for prob in y_pred_prob]"
      ],
      "metadata": {
        "id": "lHGOyZ79jzB8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}